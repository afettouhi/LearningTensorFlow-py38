{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "save_dir = \"../data/mnist\"\n",
    "\n",
    "# Download data to save_dir\n",
    "data_sets = tfds.load(name='mnist', data_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving train\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-2-e52f7e367567>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0md\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_splits\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m   \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"saving \"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mdata_splits\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 4\u001B[0;31m   \u001B[0mdata_set\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata_sets\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m   \u001B[0mfilename\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msave_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_splits\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'.tfrecords'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 0"
     ]
    }
   ],
   "source": [
    "data_splits = [\"train\",\"test\",\"validation\"]\n",
    "for d in range(len(data_splits)):\n",
    "  print(\"saving \" + data_splits[d])\n",
    "  data_set = data_sets[d]\n",
    "\n",
    "  filename = os.path.join(save_dir, data_splits[d] + '.tfrecords')\n",
    "  writer = tf.compat.v1.python_io.TFRecordWriter(filename)\n",
    "  for index in range(data_set.images.shape[0]):\n",
    "    image = data_set.images[index].tostring()\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "    'height': tf.train.Feature(int64_list=\n",
    "                                  tf.train.Int64List(value=\n",
    "                                  [data_set.images.shape[1]])),\n",
    "    'width': tf.train.Feature(int64_list=\n",
    "                                  tf.train.Int64List(value =\n",
    "                                  [data_set.images.shape[2]])),\n",
    "    'depth': tf.train.Feature(int64_list=\n",
    "                                  tf.train.Int64List(value =\n",
    "                                  [data_set.images.shape[3]])),\n",
    "    'label': tf.train.Feature(int64_list=\n",
    "                                  tf.train.Int64List(value =\n",
    "                                  [int(data_set.labels[index])])),\n",
    "     'image_raw': tf.train.Feature(bytes_list=\n",
    "                                   tf.train.BytesList(value =\n",
    "                                                     [image]))}))\n",
    "    writer.write(example.SerializeToString())\n",
    "  writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "filename = os.path.join(save_dir, data_splits[d] + '.tfrecords')\n",
    "writer = tf.compat.v1.python_io.TFRecordWriter(filename)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-11a556080091>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mimage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata_set\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimages\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtostring\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'data_set' is not defined"
     ]
    }
   ],
   "source": [
    "image = data_set.images[index].tostring()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-5-dd3197899ff1>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m tf.train.Feature(int64_list=tf.train.Int64List(value =\n\u001B[0;32m----> 2\u001B[0;31m                                   [int(data_set.labels[index])]))\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'data_set' is not defined"
     ]
    }
   ],
   "source": [
    "tf.train.Feature(int64_list=tf.train.Int64List(value =\n",
    "                                  [int(data_set.labels[index])]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-4469d006517c>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mFeature\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbytes_list\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mBytesList\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mimage\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'image' is not defined"
     ]
    }
   ],
   "source": [
    "tf.train.Feature(bytes_list=tf.train.BytesList(value =[image]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-1d4607501fa1>:2: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-7-1d4607501fa1>:2: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use eager execution and: \n",
      "`tf.data.TFRecordDataset(path)`\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mStopIteration\u001B[0m                             Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-1d4607501fa1>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mfilename\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msave_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'train.tfrecords'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0mrecord_iterator\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcompat\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mv1\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpython_io\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtf_record_iterator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mseralized_img_example\u001B[0m\u001B[0;34m=\u001B[0m \u001B[0mnext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrecord_iterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mStopIteration\u001B[0m: "
     ]
    }
   ],
   "source": [
    "filename = os.path.join(save_dir, 'train.tfrecords')\n",
    "record_iterator = tf.compat.v1.python_io.tf_record_iterator(filename)\n",
    "seralized_img_example= next(record_iterator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seralized_img_example' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-43a69caff150>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mexample\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mExample\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mexample\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mParseFromString\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mseralized_img_example\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mimage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mexample\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeature\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'image_raw'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbytes_list\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mlabel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mexample\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeature\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'label'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mint64_list\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mwidth\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mexample\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfeature\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'width'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mint64_list\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalue\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'seralized_img_example' is not defined"
     ]
    }
   ],
   "source": [
    "example = tf.train.Example()\n",
    "example.ParseFromString(seralized_img_example)\n",
    "image = example.features.feature['image_raw'].bytes_list.value\n",
    "label = example.features.feature['label'].int64_list.value[0]\n",
    "width = example.features.feature['width'].int64_list.value[0]\n",
    "height = example.features.feature['height'].int64_list.value[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-9-513af0e9ccdf>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mimg_flat\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfromstring\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimage\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muint8\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0mimg_reshaped\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mimg_flat\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mheight\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwidth\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'image' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "img_flat = np.fromstring(image[0], dtype=np.uint8)\n",
    "img_reshaped = img_flat.reshape((height, width, -1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sess= tf.compat.v1.InteractiveSession()\n",
    "queue1 = tf.compat.v1.FIFOQueue(capacity=10,dtypes=[tf.string])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "enque_op = queue1.enqueue([\"F\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(queue1.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enque_op.run()\n",
    "sess.run(queue1.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "4"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enque_op = queue1.enqueue([\"I\"])\n",
    "enque_op.run()\n",
    "enque_op = queue1.enqueue([\"F\"])\n",
    "enque_op.run()\n",
    "enque_op = queue1.enqueue([\"O\"])\n",
    "enque_op.run()\n",
    "sess.run(queue1.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "b'F'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = queue1.dequeue()\n",
    "x.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "b'I'"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "b'F'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "b'O'"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "queue1 = tf.compat.v1.FIFOQueue(capacity=10,dtypes=[tf.string],shapes=[()])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs = queue1.dequeue_many(4)\n",
    "inputs.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import threading\n",
    "import time\n",
    "\n",
    "gen_random_normal = tf.compat.v1.random_normal(shape=())\n",
    "queue = tf.compat.v1.FIFOQueue(capacity=100,dtypes=[tf.float32],shapes=())\n",
    "enque = queue.enqueue(gen_random_normal)\n",
    "\n",
    "def add():\n",
    "    for i in range(10):\n",
    "        sess.run(enque)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "[<Thread(Thread-4, initial)>,\n <Thread(Thread-5, initial)>,\n <Thread(Thread-6, initial)>,\n <Thread(Thread-7, initial)>,\n <Thread(Thread-8, initial)>,\n <Thread(Thread-9, initial)>,\n <Thread(Thread-10, initial)>,\n <Thread(Thread-11, initial)>,\n <Thread(Thread-12, initial)>,\n <Thread(Thread-13, initial)>]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threads = [threading.Thread(target=add, args=()) for i in range(10)]\n",
    "threads"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "for t in threads:\n",
    "    t.start()\n",
    "\n",
    "print(sess.run(queue.size()))\n",
    "time.sleep(0.01)\n",
    "print(sess.run(queue.size()))\n",
    "time.sleep(0.01)\n",
    "print(sess.run(queue.size()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.13078843 -0.5178498  -1.3316205   0.49089536  1.0582721  -0.09895429\n",
      " -1.2122706   0.35055608  0.47191864 -1.1602714 ]\n"
     ]
    },
    {
     "data": {
      "text/plain": "90"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = queue.dequeue_many(10)\n",
    "print(x.eval())\n",
    "sess.run(queue.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "gen_random_normal = tf.compat.v1.random_normal(shape=())\n",
    "queue = tf.compat.v1.FIFOQueue(capacity=100,dtypes=[tf.float32],shapes=())\n",
    "enque = queue.enqueue(gen_random_normal)\n",
    "\n",
    "def add(coord,i):\n",
    "    while not coord.should_stop():\n",
    "        sess.run(enque)\n",
    "        if i == 11:\n",
    "            coord.request_stop()\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = [threading.Thread(target=add, args=(coord,i)) for i in range(10)]\n",
    "coord.join(threads)\n",
    "\n",
    "for t in threads:\n",
    "    t.start()\n",
    "\n",
    "print(sess.run(queue.size()))\n",
    "time.sleep(0.01)\n",
    "print(sess.run(queue.size()))\n",
    "time.sleep(0.01)\n",
    "print(sess.run(queue.size()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def add(coord,i):\n",
    "    while not coord.should_stop():\n",
    "        sess.run(enque)\n",
    "        if i == 1:\n",
    "            coord.request_stop()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(queue.size()))\n",
    "time.sleep(0.01)\n",
    "print(sess.run(queue.size()))\n",
    "time.sleep(0.01)\n",
    "print(sess.run(queue.size()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-27-9167c8011efe>:6: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-27-9167c8011efe>:6: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    }
   ],
   "source": [
    "gen_random_normal = tf.compat.v1.random_normal(shape=())\n",
    "queue = tf.compat.v1.RandomShuffleQueue(capacity=100,dtypes=[tf.float32],\n",
    "                              min_after_dequeue=1)\n",
    "enqueue_op = queue.enqueue(gen_random_normal)\n",
    "\n",
    "qr = tf.compat.v1.train.QueueRunner(queue, [enqueue_op] * 4)\n",
    "coord = tf.train.Coordinator()\n",
    "enqueue_threads = qr.create_threads(sess, coord=coord, start=True)\n",
    "coord.request_stop()\n",
    "coord.join(enqueue_threads)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving train\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-28-5d418c5b0ab4>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0md\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_splits\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m   \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"saving \"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mdata_splits\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 15\u001B[0;31m   \u001B[0mdata_set\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata_sets\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m   \u001B[0mfilename\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msave_dir\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata_splits\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0md\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m'.tfrecords'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: 0"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "save_dir = \"../data/mnist\"\n",
    "\n",
    "# Download data to save_dir\n",
    "data_sets = tfds.load(name='mnist', data_dir=save_dir)\n",
    "\n",
    "data_splits = [\"train\",\"test\",\"validation\"]\n",
    "for d in range(len(data_splits)):\n",
    "  print(\"saving \" + data_splits[d])\n",
    "  data_set = data_sets[d]\n",
    "\n",
    "  filename = os.path.join(save_dir, data_splits[d] + '.tfrecords')\n",
    "  writer = tf.compat.v1.python_io.TFRecordWriter(filename)\n",
    "  for index in range(data_set.images.shape[0]):\n",
    "    image = data_set.images[index].tostring()\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "    'height': tf.train.Feature(int64_list=\n",
    "                                  tf.train.Int64List(value=\n",
    "                                  [data_set.images.shape[1]])),\n",
    "    'width': tf.train.Feature(int64_list=\n",
    "                                  tf.train.Int64List(value =\n",
    "                                  [data_set.images.shape[2]])),\n",
    "    'depth': tf.train.Feature(int64_list=\n",
    "                                  tf.train.Int64List(value =\n",
    "                                  [data_set.images.shape[3]])),\n",
    "    'label': tf.train.Feature(int64_list=\n",
    "                                  tf.train.Int64List(value =\n",
    "                                  [int(data_set.labels[index])])),\n",
    "     'image_raw': tf.train.Feature(bytes_list=\n",
    "                                   tf.train.BytesList(value =\n",
    "                                                     [image]))}))\n",
    "    writer.write(example.SerializeToString())\n",
    "  writer.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-29-68768d70714f>:2: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/training/input.py:267: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/training/input.py:112: BaseResourceVariable.count_up_to (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Dataset.range instead.\n",
      "WARNING:tensorflow:From /home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/training/input.py:196: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-29-68768d70714f>:2: string_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(string_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/training/input.py:267: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/training/input.py:189: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/training/input.py:112: BaseResourceVariable.count_up_to (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Prefer Dataset.range instead.\n",
      "WARNING:tensorflow:From /home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/training/input.py:196: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    }
   ],
   "source": [
    "filename = os.path.join(save_dir ,\"train.tfrecords\")\n",
    "filename_queue = tf.compat.v1.train.string_input_producer(\n",
    "    [filename], num_epochs=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-30-408e5a691afc>:1: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-30-408e5a691afc>:1: TFRecordReader.__init__ (from tensorflow.python.ops.io_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.TFRecordDataset`.\n"
     ]
    }
   ],
   "source": [
    "reader = tf.compat.v1.TFRecordReader()\n",
    "_, serialized_example = reader.read(filename_queue)\n",
    "features = tf.compat.v1.parse_single_example(\n",
    "    serialized_example,\n",
    "    features={\n",
    "        'image_raw': tf.compat.v1.FixedLenFeature([], tf.string),\n",
    "        'label': tf.compat.v1.FixedLenFeature([], tf.int64),\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-31-e9d88aed5344>:6: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-31-e9d88aed5344>:6: shuffle_batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.shuffle(min_after_dequeue).batch(batch_size)`.\n"
     ]
    }
   ],
   "source": [
    "image = tf.compat.v1.decode_raw(features['image_raw'], tf.uint8)\n",
    "image.set_shape([784])\n",
    "image = tf.cast(image, tf.float32) * (1. / 255) - 0.5\n",
    "label = tf.cast(features['label'], tf.int32)\n",
    "# Randomly collect instances into batches\n",
    "images_batch, labels_batch = tf.compat.v1.train.shuffle_batch(\n",
    "    [image, label], batch_size=128,\n",
    "    capacity=2000,\n",
    "    min_after_dequeue=1000)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "W = tf.compat.v1.get_variable(\"W\", [28*28, 10])\n",
    "y_pred = tf.matmul(images_batch, W)\n",
    "loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y_pred,\n",
    "                                                      labels=labels_batch)\n",
    "\n",
    "loss_mean = tf.reduce_mean(loss)\n",
    "\n",
    "train_op = tf.compat.v1.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "sess.run(init)\n",
    "init = tf.compat.v1.local_variables_initializer()\n",
    "sess.run(init)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-33-9c77feb63b5a>:5: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-33-9c77feb63b5a>:5: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "# Coordinator\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.compat.v1.train.start_queue_runners(sess=sess,coord=coord)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'NUM_EPOCHS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfRangeError\u001B[0m                           Traceback (most recent call last)",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m_do_call\u001B[0;34m(self, fn, *args)\u001B[0m\n\u001B[1;32m   1364\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1365\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1366\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0merrors\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mOpError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m_run_fn\u001B[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001B[0m\n\u001B[1;32m   1348\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_extend_graph\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1349\u001B[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001B[0m\u001B[1;32m   1350\u001B[0m                                       target_list, run_metadata)\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m_call_tf_sessionrun\u001B[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001B[0m\n\u001B[1;32m   1440\u001B[0m                           run_metadata):\n\u001B[0;32m-> 1441\u001B[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001B[0m\u001B[1;32m   1442\u001B[0m                                             \u001B[0mfetch_list\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget_list\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOutOfRangeError\u001B[0m: RandomShuffleQueue '_5_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 128, current size 0)\n\t [[{{node shuffle_batch}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mOutOfRangeError\u001B[0m                           Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-34-f9db82b50686>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m       \u001B[0mstep\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m       \u001B[0msess\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtrain_op\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0mstep\u001B[0m\u001B[0;34m%\u001B[0m\u001B[0;36m500\u001B[0m\u001B[0;34m==\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[1;32m    956\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 957\u001B[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001B[0m\u001B[1;32m    958\u001B[0m                          run_metadata_ptr)\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m_run\u001B[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[1;32m   1179\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mfinal_fetches\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mfinal_targets\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mhandle\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mfeed_dict_tensor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1180\u001B[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001B[0m\u001B[1;32m   1181\u001B[0m                              feed_dict_tensor, options, run_metadata)\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m_do_run\u001B[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001B[0m\n\u001B[1;32m   1357\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mhandle\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1358\u001B[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001B[0m\u001B[1;32m   1359\u001B[0m                            run_metadata)\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m_do_call\u001B[0;34m(self, fn, *args)\u001B[0m\n\u001B[1;32m   1383\u001B[0m                     'disable_meta_optimizer = True')\n\u001B[0;32m-> 1384\u001B[0;31m       \u001B[0;32mraise\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnode_def\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mop\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmessage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1385\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mOutOfRangeError\u001B[0m: RandomShuffleQueue '_5_shuffle_batch/random_shuffle_queue' is closed and has insufficient elements (requested 128, current size 0)\n\t [[node shuffle_batch (defined at <ipython-input-31-e9d88aed5344>:6) ]]\n\nOriginal stack trace for 'shuffle_batch':\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 612, in start\n    self.io_loop.start()\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n    handle._run()\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/asyncio/events.py\", line 81, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 543, in execute_request\n    self.do_execute(\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2866, in run_cell\n    result = self._run_cell(\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2895, in _run_cell\n    return runner(coro)\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3071, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-31-e9d88aed5344>\", line 6, in <module>\n    images_batch, labels_batch = tf.compat.v1.train.shuffle_batch(\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/util/deprecation.py\", line 324, in new_func\n    return func(*args, **kwargs)\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/training/input.py\", line 1335, in shuffle_batch\n    return _shuffle_batch(\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/training/input.py\", line 874, in _shuffle_batch\n    dequeued = queue.dequeue_many(batch_size, name=name)\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/ops/data_flow_ops.py\", line 487, in dequeue_many\n    ret = gen_data_flow_ops.queue_dequeue_many_v2(\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 3534, in queue_dequeue_many_v2\n    _, _, _op, _outputs = _op_def_library._apply_op_helper(\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\", line 742, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 3319, in _create_op_internal\n    ret = Operation(\n  File \"/home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\", line 1791, in __init__\n    self._traceback = tf_stack.extract_stack()\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-34-f9db82b50686>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      9\u001B[0m           \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mloss_mean_val\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;32mexcept\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0merrors\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mOutOfRangeError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'Done training for %d epochs, %d steps.'\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mNUM_EPOCHS\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstep\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[0;31m# When done, ask the threads to stop\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'NUM_EPOCHS' is not defined"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  step = 0\n",
    "  while not coord.should_stop():\n",
    "      step += 1\n",
    "      sess.run([train_op])\n",
    "      if step%500==0:\n",
    "          loss_mean_val = sess.run([loss_mean])\n",
    "          print(step)\n",
    "          print(loss_mean_val)\n",
    "except tf.errors.OutOfRangeError:\n",
    "    print('Done training for %d epochs, %d steps.' % (NUM_EPOCHS, step))\n",
    "finally:\n",
    "    # When done, ask the threads to stop\n",
    "    coord.request_stop()\n",
    "\n",
    "# Wait for threads to finish\n",
    "coord.join(threads)\n",
    "sess.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
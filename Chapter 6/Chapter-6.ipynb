{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorboard.plugins import projector\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "\n",
    "batch_size=64\n",
    "embedding_dimension = 5\n",
    "negative_samples =8\n",
    "LOG_DIR = \"../data/logs/word2vec_intro\"\n",
    "\n",
    "\n",
    "digit_to_word_map = {1:\"One\",2:\"Two\", 3:\"Three\", 4:\"Four\", 5:\"Five\",\n",
    "                     6:\"Six\",7:\"Seven\",8:\"Eight\",9:\"Nine\"}\n",
    "sentences = []\n",
    "\n",
    "# Create two kinds of sentences - sequences of odd and even digits\n",
    "for i in range(10000):\n",
    "    rand_odd_ints = np.random.choice(range(1,10,2),3)\n",
    "    sentences.append(\" \".join([digit_to_word_map[r] for r in rand_odd_ints]))\n",
    "    rand_even_ints = np.random.choice(range(2,10,2),3)\n",
    "    sentences.append(\" \".join([digit_to_word_map[r] for r in rand_even_ints]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "['Five Three Nine',\n 'Two Two Six',\n 'Five Nine Seven',\n 'Six Two Four',\n 'One Seven Nine',\n 'Six Eight Two',\n 'One Five Three',\n 'Eight Six Two',\n 'Nine Seven Nine',\n 'Four Four Four']"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Map words to indices\n",
    "word2index_map ={}\n",
    "index=0\n",
    "for sent in sentences:\n",
    "    for word in sent.lower().split():\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index+=1\n",
    "index2word_map = {index: word for word, index in word2index_map.items()}\n",
    "vocabulary_size = len(index2word_map)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Generate skip-gram pairs\n",
    "skip_gram_pairs = []\n",
    "for sent in sentences:\n",
    "    tokenized_sent = sent.lower().split()\n",
    "    for i in range(1, len(tokenized_sent)-1) :\n",
    "        word_context_pair = [[word2index_map[tokenized_sent[i-1]],\n",
    "                              word2index_map[tokenized_sent[i+1]]],\n",
    "                              word2index_map[tokenized_sent[i]]]\n",
    "        skip_gram_pairs.append([word_context_pair[1],\n",
    "                                word_context_pair[0][0]])\n",
    "        skip_gram_pairs.append([word_context_pair[1],\n",
    "                                word_context_pair[0][1]])\n",
    "\n",
    "\n",
    "def get_skipgram_batch(batch_size):\n",
    "    instance_indices = list(range(len(skip_gram_pairs)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[:batch_size]\n",
    "    x = [skip_gram_pairs[i][0] for i in batch]\n",
    "    y = [[skip_gram_pairs[i][1]] for i in batch]\n",
    "    return x,y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "[[1, 0],\n [1, 2],\n [3, 3],\n [3, 4],\n [2, 0],\n [2, 5],\n [3, 4],\n [3, 6],\n [5, 7],\n [5, 2]]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skip_gram_pairs[0:10]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "[4, 7, 2, 1, 5, 6, 6, 3]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch example\n",
    "x_batch,y_batch = get_skipgram_batch(8)\n",
    "x_batch\n",
    "y_batch\n",
    "[index2word_map[word] for word in x_batch]\n",
    "[index2word_map[word[0]] for word in y_batch]\n",
    "\n",
    "x_batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "[[4], [5], [0], [5], [2], [8], [8], [6]]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_batch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "['six', 'one', 'nine', 'three', 'seven', 'four', 'four', 'two']"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[index2word_map[word] for word in x_batch]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "['six', 'seven', 'five', 'seven', 'nine', 'eight', 'eight', 'four']"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[index2word_map[word[0]] for word in y_batch]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Input data, labels\n",
    "train_inputs = tf.compat.v1.placeholder(tf.int32, shape=[batch_size])\n",
    "train_labels = tf.compat.v1.placeholder(tf.int32, shape=[batch_size, 1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:1659: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"embeddings\"):\n",
    "    embeddings = tf.Variable(\n",
    "        tf.compat.v1.random_uniform([vocabulary_size, embedding_dimension],\n",
    "                          -1.0, 1.0),name='embedding')\n",
    "    # This is essentially a lookup table\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Create variables for the NCE loss\n",
    "nce_weights = tf.Variable(\n",
    "        tf.compat.v1.truncated_normal([vocabulary_size, embedding_dimension],\n",
    "                            stddev=1.0 / math.sqrt(embedding_dimension)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "loss = tf.reduce_mean(\n",
    "  tf.nn.nce_loss(weights = nce_weights, biases = nce_biases, inputs = embed,\n",
    "       labels = train_labels, num_sampled = negative_samples, num_classes =\n",
    "                vocabulary_size))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Learning rate decay\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learningRate = tf.compat.v1.train.exponential_decay(learning_rate=0.1,\n",
    "                                          global_step= global_step,\n",
    "                                          decay_steps=1000,\n",
    "                                          decay_rate= 0.95,\n",
    "                                          staircase=True)\n",
    "train_step = tf.compat.v1.train.GradientDescentOptimizer(learningRate).minimize(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Fetch argument None has invalid type <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-25-4502c730e599>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     23\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mstep\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1000\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     24\u001B[0m         \u001B[0mx_batch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my_batch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_skipgram_batch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 25\u001B[0;31m         summary,_ = sess.run([merged,train_step],\n\u001B[0m\u001B[1;32m     26\u001B[0m                              feed_dict={train_inputs:x_batch,\n\u001B[1;32m     27\u001B[0m                                         train_labels:y_batch})\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[1;32m    955\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    956\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 957\u001B[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001B[0m\u001B[1;32m    958\u001B[0m                          run_metadata_ptr)\n\u001B[1;32m    959\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0mrun_metadata\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m_run\u001B[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[1;32m   1163\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1164\u001B[0m     \u001B[0;31m# Create a fetch handler to take care of the structure of fetches.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1165\u001B[0;31m     fetch_handler = _FetchHandler(\n\u001B[0m\u001B[1;32m   1166\u001B[0m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001B[1;32m   1167\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, graph, fetches, feeds, feed_handles)\u001B[0m\n\u001B[1;32m    475\u001B[0m     \"\"\"\n\u001B[1;32m    476\u001B[0m     \u001B[0;32mwith\u001B[0m \u001B[0mgraph\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mas_default\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 477\u001B[0;31m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fetch_mapper\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_FetchMapper\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfor_fetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfetches\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    478\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fetches\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    479\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_targets\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36mfor_fetch\u001B[0;34m(fetch)\u001B[0m\n\u001B[1;32m    264\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    265\u001B[0m       \u001B[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 266\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0m_ListFetchMapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    267\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcollections_abc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mMapping\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    268\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0m_DictFetchMapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, fetches)\u001B[0m\n\u001B[1;32m    376\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    377\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fetch_type\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfetches\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 378\u001B[0;31m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_mappers\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0m_FetchMapper\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfor_fetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mfetch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mfetches\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    379\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_unique_fetches\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_value_indices\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_uniquify_fetches\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_mappers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    376\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    377\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_fetch_type\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfetches\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 378\u001B[0;31m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_mappers\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0m_FetchMapper\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfor_fetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mfetch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mfetches\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    379\u001B[0m     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_unique_fetches\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_value_indices\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_uniquify_fetches\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_mappers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    380\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36mfor_fetch\u001B[0;34m(fetch)\u001B[0m\n\u001B[1;32m    260\u001B[0m     \"\"\"\n\u001B[1;32m    261\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mfetch\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 262\u001B[0;31m       raise TypeError('Fetch argument %r has invalid type %r' %\n\u001B[0m\u001B[1;32m    263\u001B[0m                       (fetch, type(fetch)))\n\u001B[1;32m    264\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: Fetch argument None has invalid type <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "# Merge all summary ops\n",
    "merged = tf.compat.v1.summary.merge_all()\n",
    "\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    train_writer = tf.compat.v1.summary.FileWriter(LOG_DIR,\n",
    "                                         graph=tf.compat.v1.get_default_graph())\n",
    "    saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "    with open(os.path.join(LOG_DIR,'metadata.tsv'), \"w\") as metadata:\n",
    "        metadata.write('Name\\tClass\\n')\n",
    "        for k,v in index2word_map.items():\n",
    "            metadata.write('%s\\t%d\\n' % (v, k))\n",
    "\n",
    "    config = projector.ProjectorConfig()\n",
    "    embedding = config.embeddings.add()\n",
    "    embedding.tensor_name = embeddings.name\n",
    "    # Link embedding to its metadata file\n",
    "    embedding.metadata_path = os.path.join(LOG_DIR,'metadata.tsv')\n",
    "    projector.visualize_embeddings(train_writer, config)\n",
    "\n",
    "    tf.compat.v1.global_variables_initializer().run()\n",
    "\n",
    "    for step in range(1000):\n",
    "        x_batch, y_batch = get_skipgram_batch(batch_size)\n",
    "        summary,_ = sess.run([merged,train_step],\n",
    "                             feed_dict={train_inputs:x_batch,\n",
    "                                        train_labels:y_batch})\n",
    "        train_writer.add_summary(summary, step)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            saver.save(sess, os.path.join(LOG_DIR, \"w2v_model.ckpt\"), step)\n",
    "            loss_value = sess.run(loss,\n",
    "                                  feed_dict={train_inputs:x_batch,\n",
    "                                             train_labels:y_batch})\n",
    "            print(\"Loss at %d: %.5f\" % (step, loss_value))\n",
    "\n",
    "    # Normalize embeddings before using\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    normalized_embeddings_matrix = sess.run(normalized_embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'normalized_embeddings_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-26-1ee7077a01de>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mref_word\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnormalized_embeddings_matrix\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mword2index_map\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"one\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0mcosine_dists\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnormalized_embeddings_matrix\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mref_word\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mff\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0margsort\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcosine_dists\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mf\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mff\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'normalized_embeddings_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "ref_word = normalized_embeddings_matrix[word2index_map[\"one\"]]\n",
    "\n",
    "cosine_dists = np.dot(normalized_embeddings_matrix,ref_word)\n",
    "ff = np.argsort(cosine_dists)[::-1][1:10]\n",
    "for f in ff:\n",
    "    print(index2word_map[f])\n",
    "    print(cosine_dists[f])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "path_to_glove = \"../data/\"\n",
    "PRE_TRAINED = True\n",
    "GLOVE_SIZE = 300\n",
    "batch_size = 128\n",
    "embedding_dimension = 64\n",
    "num_classes = 2\n",
    "hidden_layer_size = 32\n",
    "times_steps = 6"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "digit_to_word_map = {1: \"One\", 2: \"Two\", 3: \"Three\", 4: \"Four\", 5: \"Five\", 6: \"Six\", 7: \"Seven\", 8: \"Eight\", 9: \"Nine\", 0: \"PAD_TOKEN\"}\n",
    "even_sentences = []\n",
    "odd_sentences = []\n",
    "seqlens = []\n",
    "for i in range(10000):\n",
    "    rand_seq_len = np.random.choice(range(3,7))\n",
    "    seqlens.append(rand_seq_len)\n",
    "    rand_odd_ints = np.random.choice(range(1,10,2),\n",
    "                                     rand_seq_len)\n",
    "    rand_even_ints = np.random.choice(range(2,10,2),\n",
    "                                      rand_seq_len)\n",
    "    if rand_seq_len<6:\n",
    "        rand_odd_ints = np.append(rand_odd_ints,\n",
    "                                  [0]*(6-rand_seq_len))\n",
    "        rand_even_ints = np.append(rand_even_ints,\n",
    "                                   [0]*(6-rand_seq_len))\n",
    "\n",
    "    even_sentences.append(\" \".join([digit_to_word_map[r] for\n",
    "                               r in rand_odd_ints]))\n",
    "    odd_sentences.append(\" \".join([digit_to_word_map[r] for\n",
    "                              r in rand_even_ints]))\n",
    "data = even_sentences+odd_sentences\n",
    "# Same seq lengths for even, odd sentences\n",
    "seqlens*=2\n",
    "labels = [1]*10000 + [0]*10000\n",
    "for i in range(len(labels)):\n",
    "    label = labels[i]\n",
    "    one_hot_encoding = [0]*2\n",
    "    one_hot_encoding[label] = 1\n",
    "    labels[i] = one_hot_encoding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "word2index_map ={}\n",
    "index=0\n",
    "for sent in data:\n",
    "    for word in sent.split():\n",
    "        if word not in word2index_map:\n",
    "            word2index_map[word] = index\n",
    "            index+=1\n",
    "\n",
    "index2word_map = {index: word for word, index in word2index_map.items()}\n",
    "\n",
    "vocabulary_size = len(index2word_map)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "{'Seven': 0,\n 'Nine': 1,\n 'PAD_TOKEN': 2,\n 'Three': 3,\n 'One': 4,\n 'Five': 5,\n 'Six': 6,\n 'Two': 7,\n 'Eight': 8,\n 'Four': 9}"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index_map"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '../data/'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIsADirectoryError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-31-321648e6acba>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     17\u001B[0m                     \u001B[0;32mbreak\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0membedding_weights\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 19\u001B[0;31m \u001B[0mword2embedding_dict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_glove\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath_to_glove\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mword2index_map\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     20\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-31-321648e6acba>\u001B[0m in \u001B[0;36mget_glove\u001B[0;34m(path_to_glove, word2index_map)\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0membedding_weights\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0mcount_all_words\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m     \u001B[0;32mwith\u001B[0m \u001B[0mzipfile\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mZipFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath_to_glove\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mz\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m         \u001B[0;32mwith\u001B[0m \u001B[0mz\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"glove.840B.300d.txt\"\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mline\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/zipfile.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001B[0m\n\u001B[1;32m   1249\u001B[0m             \u001B[0;32mwhile\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1250\u001B[0m                 \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1251\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfp\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mio\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfile\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfilemode\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1252\u001B[0m                 \u001B[0;32mexcept\u001B[0m \u001B[0mOSError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1253\u001B[0m                     \u001B[0;32mif\u001B[0m \u001B[0mfilemode\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mmodeDict\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mIsADirectoryError\u001B[0m: [Errno 21] Is a directory: '../data/'"
     ]
    }
   ],
   "source": [
    "def get_glove(path_to_glove,word2index_map):\n",
    "\n",
    "    embedding_weights = {}\n",
    "    count_all_words = 0\n",
    "    with zipfile.ZipFile(path_to_glove) as z:\n",
    "        with z.open(\"glove.840B.300d.txt\") as f:\n",
    "            for line in f:\n",
    "                vals = line.split()\n",
    "                word = str(vals[0].decode(\"utf-8\"))\n",
    "                if word in word2index_map:\n",
    "                    print(word)\n",
    "                    count_all_words+=1\n",
    "                    coefs = np.asarray(vals[1:], dtype='float32')\n",
    "                    coefs/=np.linalg.norm(coefs)\n",
    "                    embedding_weights[word] = coefs\n",
    "                if count_all_words==vocabulary_size -1:\n",
    "                    break\n",
    "    return embedding_weights\n",
    "word2embedding_dict = get_glove(path_to_glove,word2index_map)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word2embedding_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-32-9801adc47e77>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mword\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mindex\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mword2index_map\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mword\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"PAD_TOKEN\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m         \u001B[0mword_embedding\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mword2embedding_dict\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mword\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m         \u001B[0membedding_matrix\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mword_embedding\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'word2embedding_dict' is not defined"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((vocabulary_size ,GLOVE_SIZE))\n",
    "\n",
    "for word,index in word2index_map.items():\n",
    "    if not word == \"PAD_TOKEN\":\n",
    "        word_embedding = word2embedding_dict[word]\n",
    "        embedding_matrix[index,:] = word_embedding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "data_indices = list(range(len(data)))\n",
    "np.random.shuffle(data_indices)\n",
    "data = np.array(data)[data_indices]\n",
    "labels = np.array(labels)[data_indices]\n",
    "seqlens = np.array(seqlens)[data_indices]\n",
    "train_x = data[:10000]\n",
    "train_y = labels[:10000]\n",
    "train_seqlens = seqlens[:10000]\n",
    "\n",
    "test_x = data[10000:]\n",
    "test_y = labels[10000:]\n",
    "test_seqlens = seqlens[10000:]\n",
    "\n",
    "def get_sentence_batch(batch_size,data_x,\n",
    "                       data_y,data_seqlens):\n",
    "    instance_indices = list(range(len(data_x)))\n",
    "    np.random.shuffle(instance_indices)\n",
    "    batch = instance_indices[:batch_size]\n",
    "    x = [[word2index_map[word] for word in data_x[i].split()]\n",
    "         for i in batch]\n",
    "    y = [data_y[i] for i in batch]\n",
    "    seqlens = [data_seqlens[i] for i in batch]\n",
    "    return x,y,seqlens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "_inputs = tf.compat.v1.placeholder(tf.int32, shape=[batch_size,times_steps])\n",
    "embedding_placeholder = tf.compat.v1.placeholder(tf.float32, [vocabulary_size,\n",
    "                                                    GLOVE_SIZE])\n",
    "\n",
    "_labels = tf.compat.v1.placeholder(tf.float32, shape=[batch_size, num_classes])\n",
    "_seqlens = tf.compat.v1.placeholder(tf.int32, shape=[batch_size])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "if PRE_TRAINED:\n",
    "\n",
    "        embeddings = tf.Variable(tf.constant(0.0, shape=[vocabulary_size,\n",
    "                                 GLOVE_SIZE]),\n",
    "                                 trainable=True)\n",
    "        # If using pretrained embeddings, assign them to the embeddings variable\n",
    "        embedding_init = embeddings.assign(embedding_placeholder)\n",
    "        embed = tf.nn.embedding_lookup(embeddings, _inputs)\n",
    "\n",
    "else:\n",
    "        embeddings = tf.Variable(\n",
    "            tf.compat.v1.random_uniform([vocabulary_size,\n",
    "                               embedding_dimension],\n",
    "                              -1.0, 1.0))\n",
    "        embed = tf.nn.embedding_lookup(embeddings, _inputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-36-b0e208c76f70>:3: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-36-b0e208c76f70>:10: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/ops/rnn.py:455: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/ops/rnn_cell_impl.py:561: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/ops/rnn_cell_impl.py:570: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/ops/rnn_cell_impl.py:580: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"biGRU\"):\n",
    "    with tf.compat.v1.variable_scope('forward'):\n",
    "        gru_fw_cell = tf.compat.v1.nn.rnn_cell.GRUCell(hidden_layer_size)\n",
    "        gru_fw_cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(gru_fw_cell)\n",
    "\n",
    "    with tf.compat.v1.variable_scope('backward'):\n",
    "        gru_bw_cell = tf.compat.v1.nn.rnn_cell.GRUCell(hidden_layer_size)\n",
    "        gru_bw_cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(gru_bw_cell)\n",
    "\n",
    "    outputs, states = tf.compat.v1.nn.bidirectional_dynamic_rnn(cell_fw=gru_fw_cell,\n",
    "                                                      cell_bw=gru_bw_cell,\n",
    "                                                      inputs=embed,\n",
    "                                                      sequence_length=\n",
    "                                                      _seqlens,\n",
    "                                                      dtype=tf.float32,\n",
    "                                                      scope=\"BiGRU\")\n",
    "states = tf.concat(values=states, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/af/Dokumenter/Programs/miniconda3/envs/LearningTensorFlow/lib/python3.8/site-packages/tensorflow/python/training/rmsprop.py:123: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "weights = {\n",
    "    'linear_layer': tf.Variable(tf.compat.v1.truncated_normal([2*hidden_layer_size,\n",
    "                                                     num_classes],\n",
    "                                                     mean=0,stddev=.01))\n",
    "}\n",
    "biases = {\n",
    "    'linear_layer':tf.Variable(tf.compat.v1.truncated_normal([num_classes],\n",
    "                                                   mean=0,stddev=.01))\n",
    "}\n",
    "\n",
    "# extract the final state and use in a linear layer\n",
    "final_output = tf.matmul(states,\n",
    "                         weights[\"linear_layer\"]) + biases[\"linear_layer\"]\n",
    "\n",
    "softmax = tf.nn.softmax_cross_entropy_with_logits(logits=final_output,\n",
    "                                                  labels=_labels)\n",
    "cross_entropy = tf.reduce_mean(softmax)\n",
    "\n",
    "train_step = tf.compat.v1.train.RMSPropOptimizer(0.001, 0.9).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(_labels,1),\n",
    "                              tf.argmax(final_output,1))\n",
    "accuracy = (tf.reduce_mean(tf.cast(correct_prediction,\n",
    "                                   tf.float32)))*100"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy at 0: 45.31250\n",
      "Accuracy at 100: 47.65625\n",
      "Accuracy at 200: 100.00000\n",
      "Accuracy at 300: 100.00000\n",
      "Accuracy at 400: 100.00000\n",
      "Accuracy at 500: 100.00000\n",
      "Accuracy at 600: 100.00000\n",
      "Accuracy at 700: 100.00000\n",
      "Accuracy at 800: 100.00000\n",
      "Accuracy at 900: 100.00000\n",
      "Test batch accuracy 0: 100.00000\n",
      "Test batch accuracy 0: 100.00000\n",
      "Test batch accuracy 1: 100.00000\n",
      "Test batch accuracy 1: 100.00000\n",
      "Test batch accuracy 2: 100.00000\n",
      "Test batch accuracy 2: 100.00000\n",
      "Test batch accuracy 3: 100.00000\n",
      "Test batch accuracy 3: 100.00000\n",
      "Test batch accuracy 4: 100.00000\n",
      "Test batch accuracy 4: 100.00000\n"
     ]
    }
   ],
   "source": [
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    sess.run(embedding_init, feed_dict=\n",
    "             {embedding_placeholder: embedding_matrix})\n",
    "    for step in range(1000):\n",
    "        x_batch, y_batch,seqlen_batch = get_sentence_batch(batch_size,\n",
    "                                                           train_x,train_y,\n",
    "                                                           train_seqlens)\n",
    "        sess.run(train_step,feed_dict={_inputs:x_batch, _labels:y_batch,\n",
    "                                       _seqlens:seqlen_batch})\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            acc = sess.run(accuracy,feed_dict={_inputs:x_batch,\n",
    "                                               _labels:y_batch,\n",
    "                                               _seqlens:seqlen_batch})\n",
    "            print(\"Accuracy at %d: %.5f\" % (step, acc))\n",
    "\n",
    "    for test_batch in range(5):\n",
    "        x_test, y_test,seqlen_test = get_sentence_batch(batch_size,\n",
    "                                                        test_x,test_y,\n",
    "                                                        test_seqlens)\n",
    "        batch_pred,batch_acc = sess.run([tf.argmax(final_output,1),\n",
    "                                         accuracy],\n",
    "                                        feed_dict={_inputs:x_test,\n",
    "                                                   _labels:y_test,\n",
    "                                                   _seqlens:seqlen_test})\n",
    "        print(\"Test batch accuracy %d: %.5f\" % (test_batch, batch_acc))\n",
    "        print(\"Test batch accuracy %d: %.5f\" % (test_batch, batch_acc))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}